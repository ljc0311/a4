"""
æ™ºèƒ½åŒæ­¥æ£€æµ‹å™¨
æ›¿æ¢æ—§çš„ç®€å•æ•°é‡æ£€æµ‹ï¼Œæä¾›æ›´æ™ºèƒ½çš„é…éŸ³-å›¾åƒåŒæ­¥åˆ†æ
"""

import os
import json
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

from src.utils.logger import logger
from src.utils.audio_duration_analyzer import AudioDurationAnalyzer
from src.core.voice_image_matcher import VoiceImageMatcher


@dataclass
class SyncIssue:
    """åŒæ­¥é—®é¢˜æ•°æ®ç»“æ„"""
    issue_type: str  # 'duration_mismatch', 'content_mismatch', 'count_mismatch', 'quality_issue'
    severity: str  # 'low', 'medium', 'high', 'critical'
    description: str
    affected_segments: List[int]
    suggested_fix: str
    auto_fixable: bool = False


@dataclass
class SyncAnalysisResult:
    """åŒæ­¥åˆ†æç»“æœ"""
    overall_quality: float  # 0.0-1.0
    issues: List[SyncIssue]
    recommendations: List[str]
    voice_segments_count: int
    image_segments_count: int
    total_voice_duration: float
    estimated_video_duration: float
    sync_score: float  # 0.0-1.0


class IntelligentSyncDetector:
    """æ™ºèƒ½åŒæ­¥æ£€æµ‹å™¨"""
    
    def __init__(self, project_manager=None):
        self.project_manager = project_manager
        self.audio_analyzer = AudioDurationAnalyzer()
        self.voice_matcher = VoiceImageMatcher()
        
        # æ£€æµ‹é˜ˆå€¼é…ç½®
        self.thresholds = {
            'min_segment_duration': 1.0,  # æœ€çŸ­æ®µè½æ—¶é•¿
            'max_segment_duration': 15.0,  # æœ€é•¿æ®µè½æ—¶é•¿
            'ideal_segment_duration': 5.0,  # ç†æƒ³æ®µè½æ—¶é•¿
            'duration_variance_threshold': 0.3,  # æ—¶é•¿æ–¹å·®é˜ˆå€¼
            'content_similarity_threshold': 0.7,  # å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
            'sync_quality_threshold': 0.8,  # åŒæ­¥è´¨é‡é˜ˆå€¼
        }
    
    def analyze_project_sync(self, project_data: Dict[str, Any]) -> SyncAnalysisResult:
        """åˆ†æé¡¹ç›®çš„é…éŸ³-å›¾åƒåŒæ­¥çŠ¶æ€"""
        try:
            logger.info("å¼€å§‹æ™ºèƒ½åŒæ­¥åˆ†æ...")
            
            # æå–é…éŸ³æ•°æ®
            voice_data = self._extract_voice_data(project_data)
            
            # æå–å›¾åƒæ•°æ®
            image_data = self._extract_image_data(project_data)
            
            # åˆ†ææ—¶é•¿åŒ¹é…
            duration_issues = self._analyze_duration_sync(voice_data, image_data)
            
            # åˆ†æå†…å®¹åŒ¹é…
            content_issues = self._analyze_content_sync(voice_data, image_data)
            
            # åˆ†ææ•°é‡åŒ¹é…
            count_issues = self._analyze_count_sync(voice_data, image_data)
            
            # åˆ†æè´¨é‡é—®é¢˜
            quality_issues = self._analyze_quality_issues(voice_data, image_data)
            
            # æ±‡æ€»æ‰€æœ‰é—®é¢˜
            all_issues = duration_issues + content_issues + count_issues + quality_issues
            
            # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
            overall_quality = self._calculate_overall_quality(all_issues, voice_data, image_data)
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self._generate_recommendations(all_issues, voice_data, image_data)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            voice_count = len(voice_data)
            image_count = len(image_data)
            total_duration = sum(self.audio_analyzer.analyze_duration(
                v.get('audio_path', ''), v.get('dialogue_text', '')
            ) for v in voice_data)
            
            result = SyncAnalysisResult(
                overall_quality=overall_quality,
                issues=all_issues,
                recommendations=recommendations,
                voice_segments_count=voice_count,
                image_segments_count=image_count,
                total_voice_duration=total_duration,
                estimated_video_duration=total_duration,
                sync_score=self._calculate_sync_score(all_issues)
            )
            
            logger.info(f"åŒæ­¥åˆ†æå®Œæˆï¼Œè´¨é‡åˆ†æ•°: {overall_quality:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åŒæ­¥åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return SyncAnalysisResult(
                overall_quality=0.5,
                issues=[],
                recommendations=["åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é¡¹ç›®æ•°æ®"],
                voice_segments_count=0,
                image_segments_count=0,
                total_voice_duration=0.0,
                estimated_video_duration=0.0,
                sync_score=0.5
            )
    
    def _extract_voice_data(self, project_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–é…éŸ³æ•°æ®"""
        voice_data = []
        
        # ä»é…éŸ³ç”Ÿæˆæ¨¡å—è·å–æ•°æ®
        voice_generation = project_data.get('voice_generation', {})
        
        # ä¼˜å…ˆä½¿ç”¨å·²ç”Ÿæˆçš„éŸ³é¢‘
        generated_audio = voice_generation.get('generated_audio', [])
        if generated_audio:
            voice_data.extend(generated_audio)
        
        # å¤‡é€‰ï¼šä½¿ç”¨é…éŸ³æ®µè½
        voice_segments = voice_generation.get('voice_segments', [])
        if voice_segments and not generated_audio:
            voice_data.extend(voice_segments)
        
        return voice_data
    
    def _extract_image_data(self, project_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–å›¾åƒæ•°æ®"""
        image_data = []
        
        # ä»å›¾åƒç”Ÿæˆæ¨¡å—è·å–æ•°æ®
        image_generation = project_data.get('image_generation', {})
        generated_images = image_generation.get('generated_images', [])
        
        if generated_images:
            image_data.extend(generated_images)
        
        # å¤‡é€‰ï¼šä»åˆ†é•œæ•°æ®è·å–
        storyboard_data = project_data.get('storyboard_data', [])
        if storyboard_data and not generated_images:
            image_data.extend(storyboard_data)
        
        return image_data
    
    def _analyze_duration_sync(self, voice_data: List[Dict], image_data: List[Dict]) -> List[SyncIssue]:
        """åˆ†ææ—¶é•¿åŒæ­¥é—®é¢˜"""
        issues = []
        
        try:
            # åˆ†æé…éŸ³æ—¶é•¿åˆ†å¸ƒ
            durations = []
            for voice_segment in voice_data:
                duration = self.audio_analyzer.analyze_duration(
                    voice_segment.get('audio_path', ''),
                    voice_segment.get('dialogue_text', '')
                )
                durations.append(duration)
            
            if not durations:
                return issues
            
            # æ£€æŸ¥è¿‡çŸ­çš„æ®µè½
            short_segments = [i for i, d in enumerate(durations) 
                            if d < self.thresholds['min_segment_duration']]
            if short_segments:
                issues.append(SyncIssue(
                    issue_type='duration_mismatch',
                    severity='medium',
                    description=f"å‘ç° {len(short_segments)} ä¸ªè¿‡çŸ­çš„é…éŸ³æ®µè½ï¼ˆ<{self.thresholds['min_segment_duration']}ç§’ï¼‰",
                    affected_segments=short_segments,
                    suggested_fix="å»ºè®®åˆå¹¶ç›¸é‚»çš„çŸ­æ®µè½æˆ–å¢åŠ é…éŸ³å†…å®¹",
                    auto_fixable=True
                ))
            
            # æ£€æŸ¥è¿‡é•¿çš„æ®µè½
            long_segments = [i for i, d in enumerate(durations) 
                           if d > self.thresholds['max_segment_duration']]
            if long_segments:
                issues.append(SyncIssue(
                    issue_type='duration_mismatch',
                    severity='medium',
                    description=f"å‘ç° {len(long_segments)} ä¸ªè¿‡é•¿çš„é…éŸ³æ®µè½ï¼ˆ>{self.thresholds['max_segment_duration']}ç§’ï¼‰",
                    affected_segments=long_segments,
                    suggested_fix="å»ºè®®åˆ†å‰²é•¿æ®µè½æˆ–å¢åŠ æ›´å¤šå›¾åƒå˜åŒ–",
                    auto_fixable=True
                ))
            
            # æ£€æŸ¥æ—¶é•¿æ–¹å·®
            if len(durations) > 1:
                avg_duration = sum(durations) / len(durations)
                variance = sum((d - avg_duration) ** 2 for d in durations) / len(durations)
                if variance > self.thresholds['duration_variance_threshold'] * avg_duration:
                    issues.append(SyncIssue(
                        issue_type='duration_mismatch',
                        severity='low',
                        description=f"é…éŸ³æ®µè½æ—¶é•¿å·®å¼‚è¾ƒå¤§ï¼ˆæ–¹å·®: {variance:.2f}ï¼‰",
                        affected_segments=list(range(len(durations))),
                        suggested_fix="å»ºè®®è°ƒæ•´é…éŸ³æ®µè½é•¿åº¦ï¼Œä½¿å…¶æ›´åŠ å‡åŒ€",
                        auto_fixable=False
                    ))
            
        except Exception as e:
            logger.error(f"æ—¶é•¿åŒæ­¥åˆ†æå¤±è´¥: {e}")
        
        return issues
    
    def _analyze_content_sync(self, voice_data: List[Dict], image_data: List[Dict]) -> List[SyncIssue]:
        """åˆ†æå†…å®¹åŒæ­¥é—®é¢˜"""
        issues = []
        
        try:
            # æ£€æŸ¥é…éŸ³ä¸å›¾åƒå†…å®¹çš„åŒ¹é…åº¦
            mismatched_segments = []
            
            min_count = min(len(voice_data), len(image_data))
            for i in range(min_count):
                voice_content = voice_data[i].get('dialogue_text', '')
                image_description = image_data[i].get('enhanced_description', 
                                                    image_data[i].get('consistency_description', ''))
                
                if voice_content and image_description:
                    # ç®€å•çš„å†…å®¹åŒ¹é…æ£€æŸ¥ï¼ˆå¯ä»¥åç»­ç”¨LLMå¢å¼ºï¼‰
                    similarity = self._calculate_content_similarity(voice_content, image_description)
                    if similarity < self.thresholds['content_similarity_threshold']:
                        mismatched_segments.append(i)
            
            if mismatched_segments:
                issues.append(SyncIssue(
                    issue_type='content_mismatch',
                    severity='high',
                    description=f"å‘ç° {len(mismatched_segments)} ä¸ªé…éŸ³ä¸å›¾åƒå†…å®¹ä¸åŒ¹é…çš„æ®µè½",
                    affected_segments=mismatched_segments,
                    suggested_fix="å»ºè®®ä½¿ç”¨'æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒ'åŠŸèƒ½é‡æ–°ç”ŸæˆåŒ¹é…çš„å›¾åƒ",
                    auto_fixable=True
                ))
            
        except Exception as e:
            logger.error(f"å†…å®¹åŒæ­¥åˆ†æå¤±è´¥: {e}")
        
        return issues
    
    def _analyze_count_sync(self, voice_data: List[Dict], image_data: List[Dict]) -> List[SyncIssue]:
        """åˆ†ææ•°é‡åŒæ­¥é—®é¢˜"""
        issues = []
        
        voice_count = len(voice_data)
        image_count = len(image_data)
        
        if voice_count != image_count:
            severity = 'high' if abs(voice_count - image_count) > 2 else 'medium'
            issues.append(SyncIssue(
                issue_type='count_mismatch',
                severity=severity,
                description=f"é…éŸ³æ®µè½æ•°é‡ï¼ˆ{voice_count}ï¼‰ä¸å›¾åƒæ•°é‡ï¼ˆ{image_count}ï¼‰ä¸åŒ¹é…",
                affected_segments=list(range(max(voice_count, image_count))),
                suggested_fix="å»ºè®®ä½¿ç”¨'æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒ'åŠŸèƒ½è‡ªåŠ¨åŒ¹é…æ•°é‡",
                auto_fixable=True
            ))
        
        return issues
    
    def _analyze_quality_issues(self, voice_data: List[Dict], image_data: List[Dict]) -> List[SyncIssue]:
        """åˆ†æè´¨é‡é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥ç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶
        missing_audio = []
        for i, voice_segment in enumerate(voice_data):
            audio_path = voice_segment.get('audio_path', '')
            if not audio_path or not os.path.exists(audio_path):
                missing_audio.append(i)
        
        if missing_audio:
            issues.append(SyncIssue(
                issue_type='quality_issue',
                severity='critical',
                description=f"å‘ç° {len(missing_audio)} ä¸ªç¼ºå¤±çš„éŸ³é¢‘æ–‡ä»¶",
                affected_segments=missing_audio,
                suggested_fix="è¯·é‡æ–°ç”Ÿæˆç¼ºå¤±çš„é…éŸ³æ–‡ä»¶",
                auto_fixable=False
            ))
        
        # æ£€æŸ¥ç¼ºå¤±çš„å›¾åƒæ–‡ä»¶
        missing_images = []
        for i, image_segment in enumerate(image_data):
            image_path = image_segment.get('image_path', image_segment.get('main_image_path', ''))
            if not image_path or not os.path.exists(image_path):
                missing_images.append(i)
        
        if missing_images:
            issues.append(SyncIssue(
                issue_type='quality_issue',
                severity='critical',
                description=f"å‘ç° {len(missing_images)} ä¸ªç¼ºå¤±çš„å›¾åƒæ–‡ä»¶",
                affected_segments=missing_images,
                suggested_fix="è¯·é‡æ–°ç”Ÿæˆç¼ºå¤±çš„å›¾åƒæ–‡ä»¶",
                auto_fixable=False
            ))
        
        return issues

    def _calculate_content_similarity(self, voice_content: str, image_description: str) -> float:
        """è®¡ç®—é…éŸ³å†…å®¹ä¸å›¾åƒæè¿°çš„ç›¸ä¼¼åº¦"""
        try:
            # ç®€å•çš„å…³é”®è¯åŒ¹é…ç®—æ³•ï¼ˆå¯ä»¥åç»­ç”¨æ›´é«˜çº§çš„NLPæ–¹æ³•ï¼‰
            voice_keywords = set(voice_content.lower().split())
            image_keywords = set(image_description.lower().split())

            if not voice_keywords or not image_keywords:
                return 0.0

            # è®¡ç®—äº¤é›†æ¯”ä¾‹
            intersection = voice_keywords.intersection(image_keywords)
            union = voice_keywords.union(image_keywords)

            return len(intersection) / len(union) if union else 0.0

        except Exception as e:
            logger.error(f"è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰ç›¸ä¼¼åº¦

    def _calculate_overall_quality(self, issues: List[SyncIssue], voice_data: List[Dict], image_data: List[Dict]) -> float:
        """è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°"""
        try:
            if not voice_data and not image_data:
                return 0.0

            # åŸºç¡€åˆ†æ•°
            base_score = 1.0

            # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦æ‰£åˆ†
            for issue in issues:
                if issue.severity == 'critical':
                    base_score -= 0.3
                elif issue.severity == 'high':
                    base_score -= 0.2
                elif issue.severity == 'medium':
                    base_score -= 0.1
                elif issue.severity == 'low':
                    base_score -= 0.05

            # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
            return max(0.0, min(1.0, base_score))

        except Exception as e:
            logger.error(f"è®¡ç®—æ•´ä½“è´¨é‡å¤±è´¥: {e}")
            return 0.5

    def _calculate_sync_score(self, issues: List[SyncIssue]) -> float:
        """è®¡ç®—åŒæ­¥åˆ†æ•°"""
        try:
            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼ŒåŒæ­¥åˆ†æ•°ä¸º1.0
            if not issues:
                return 1.0

            # æ ¹æ®å¯è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜æ¯”ä¾‹è®¡ç®—åˆ†æ•°
            auto_fixable_count = sum(1 for issue in issues if issue.auto_fixable)
            total_count = len(issues)

            # å¯è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜å½±å“è¾ƒå°
            auto_fixable_penalty = auto_fixable_count * 0.1
            manual_fix_penalty = (total_count - auto_fixable_count) * 0.3

            score = 1.0 - (auto_fixable_penalty + manual_fix_penalty)
            return max(0.0, min(1.0, score))

        except Exception as e:
            logger.error(f"è®¡ç®—åŒæ­¥åˆ†æ•°å¤±è´¥: {e}")
            return 0.5

    def _generate_recommendations(self, issues: List[SyncIssue], voice_data: List[Dict], image_data: List[Dict]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        try:
            # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
            issue_types = set(issue.issue_type for issue in issues)

            if 'count_mismatch' in issue_types:
                recommendations.append("ğŸµ ä½¿ç”¨'æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒ'åŠŸèƒ½è‡ªåŠ¨åŒ¹é…é…éŸ³ä¸å›¾åƒæ•°é‡")

            if 'content_mismatch' in issue_types:
                recommendations.append("ğŸ¯ ä½¿ç”¨é…éŸ³ä¼˜å…ˆå·¥ä½œæµç¨‹é‡æ–°ç”ŸæˆåŒ¹é…çš„å›¾åƒå†…å®¹")

            if 'duration_mismatch' in issue_types:
                recommendations.append("â±ï¸ è°ƒæ•´é…éŸ³æ®µè½æ—¶é•¿ï¼Œå»ºè®®æ¯æ®µ3-8ç§’ä¸ºæœ€ä½³")

            if 'quality_issue' in issue_types:
                recommendations.append("ğŸ”§ ä¿®å¤ç¼ºå¤±çš„éŸ³é¢‘æˆ–å›¾åƒæ–‡ä»¶")

            # æ ¹æ®æ•°æ®çŠ¶æ€ç”Ÿæˆé€šç”¨å»ºè®®
            voice_count = len(voice_data)
            image_count = len(image_data)

            if voice_count > 0 and image_count == 0:
                recommendations.append("ğŸ“¸ å»ºè®®å…ˆç”Ÿæˆå›¾åƒï¼Œç„¶åä½¿ç”¨æ™ºèƒ½åŒæ­¥åŠŸèƒ½")
            elif voice_count == 0 and image_count > 0:
                recommendations.append("ğŸ¤ å»ºè®®å…ˆç”Ÿæˆé…éŸ³ï¼Œç„¶åä½¿ç”¨æ™ºèƒ½åŒæ­¥åŠŸèƒ½")
            elif voice_count > 0 and image_count > 0:
                # è®¡ç®—æ€»æ—¶é•¿
                total_duration = sum(self.audio_analyzer.analyze_duration(
                    v.get('audio_path', ''), v.get('dialogue_text', '')
                ) for v in voice_data)

                if total_duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                    recommendations.append("ğŸ¬ è§†é¢‘è¾ƒé•¿ï¼Œå»ºè®®åˆ†æ®µå¤„ç†ä»¥æé«˜è´¨é‡")

                # æ£€æŸ¥å›¾åƒå¯†åº¦
                if image_count < voice_count * 0.8:
                    recommendations.append("ğŸ–¼ï¸ å›¾åƒæ•°é‡åå°‘ï¼Œå»ºè®®å¢åŠ å›¾åƒä»¥ä¸°å¯Œè§†è§‰æ•ˆæœ")

            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œç»™å‡ºä¼˜åŒ–å»ºè®®
            if not issues:
                recommendations.append("âœ… é…éŸ³ä¸å›¾åƒåŒæ­¥è‰¯å¥½ï¼Œå¯ä»¥å¼€å§‹è§†é¢‘åˆæˆ")
                recommendations.append("ğŸ’¡ å»ºè®®é¢„è§ˆæ•ˆæœï¼Œç¡®è®¤æ»¡æ„åå†è¿›è¡Œæœ€ç»ˆæ¸²æŸ“")

        except Exception as e:
            logger.error(f"ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            recommendations.append("âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥é¡¹ç›®æ•°æ®")

        return recommendations

    def get_auto_fix_suggestions(self, issues: List[SyncIssue]) -> List[Dict[str, Any]]:
        """è·å–è‡ªåŠ¨ä¿®å¤å»ºè®®"""
        suggestions = []

        for issue in issues:
            if issue.auto_fixable:
                suggestion = {
                    'issue_type': issue.issue_type,
                    'description': issue.description,
                    'fix_action': self._get_fix_action(issue),
                    'affected_segments': issue.affected_segments,
                    'estimated_time': self._estimate_fix_time(issue)
                }
                suggestions.append(suggestion)

        return suggestions

    def _get_fix_action(self, issue: SyncIssue) -> str:
        """è·å–ä¿®å¤åŠ¨ä½œ"""
        if issue.issue_type == 'count_mismatch':
            return 'regenerate_images_by_voice_time'
        elif issue.issue_type == 'content_mismatch':
            return 'regenerate_matched_images'
        elif issue.issue_type == 'duration_mismatch':
            if 'short' in issue.description.lower():
                return 'merge_short_segments'
            elif 'long' in issue.description.lower():
                return 'split_long_segments'
            else:
                return 'adjust_segment_durations'
        else:
            return 'manual_fix_required'

    def _estimate_fix_time(self, issue: SyncIssue) -> str:
        """ä¼°ç®—ä¿®å¤æ—¶é—´"""
        if issue.issue_type == 'count_mismatch':
            return "2-5åˆ†é’Ÿ"
        elif issue.issue_type == 'content_mismatch':
            return "5-10åˆ†é’Ÿ"
        elif issue.issue_type == 'duration_mismatch':
            return "1-3åˆ†é’Ÿ"
        else:
            return "éœ€è¦æ‰‹åŠ¨å¤„ç†"
