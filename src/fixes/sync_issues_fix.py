"""
åŒæ­¥é—®é¢˜ä¿®å¤è„šæœ¬
è§£å†³é…éŸ³æ—¶é•¿æ£€æµ‹é”™è¯¯ã€æ®µè½æ•°é‡å˜åŒ–ã€æè¿°å†…å®¹é”™è¯¯ç­‰é—®é¢˜
"""

import os
import json
from typing import Dict, List, Any, Optional
from pathlib import Path

from src.utils.logger import logger
from src.utils.audio_duration_analyzer import AudioDurationAnalyzer


class SyncIssuesFixer:
    """åŒæ­¥é—®é¢˜ä¿®å¤å™¨"""
    
    def __init__(self, project_manager=None):
        self.project_manager = project_manager
        self.audio_analyzer = AudioDurationAnalyzer()
    
    def fix_audio_duration_detection(self, project_data: Dict[str, Any]) -> bool:
        """ä¿®å¤éŸ³é¢‘æ—¶é•¿æ£€æµ‹é”™è¯¯"""
        try:
            logger.info("å¼€å§‹ä¿®å¤éŸ³é¢‘æ—¶é•¿æ£€æµ‹é—®é¢˜...")
            
            voice_generation = project_data.get('voice_generation', {})
            generated_audio = voice_generation.get('generated_audio', [])
            voice_segments = voice_generation.get('voice_segments', [])
            
            # ä¿®å¤ç”Ÿæˆçš„éŸ³é¢‘æ—¶é•¿
            fixed_audio = []
            for audio_data in generated_audio:
                audio_path = audio_data.get('audio_path', '')
                if audio_path and os.path.exists(audio_path):
                    # é‡æ–°åˆ†æçœŸå®æ—¶é•¿
                    real_duration = self._get_real_audio_duration(audio_path)
                    if real_duration > 0:
                        audio_data['duration'] = real_duration
                        logger.info(f"ä¿®å¤éŸ³é¢‘æ—¶é•¿: {audio_path} -> {real_duration:.2f}ç§’")
                
                fixed_audio.append(audio_data)
            
            # ä¿®å¤é…éŸ³æ®µè½æ—¶é•¿
            fixed_segments = []
            for segment in voice_segments:
                audio_path = segment.get('audio_path', '')
                if audio_path and os.path.exists(audio_path):
                    # é‡æ–°åˆ†æçœŸå®æ—¶é•¿
                    real_duration = self._get_real_audio_duration(audio_path)
                    if real_duration > 0:
                        segment['duration'] = real_duration
                        logger.info(f"ä¿®å¤æ®µè½æ—¶é•¿: {segment.get('shot_id', 'Unknown')} -> {real_duration:.2f}ç§’")
                
                fixed_segments.append(segment)
            
            # æ›´æ–°é¡¹ç›®æ•°æ®
            voice_generation['generated_audio'] = fixed_audio
            voice_generation['voice_segments'] = fixed_segments
            project_data['voice_generation'] = voice_generation
            
            logger.info("éŸ³é¢‘æ—¶é•¿æ£€æµ‹ä¿®å¤å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿®å¤éŸ³é¢‘æ—¶é•¿æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def fix_segment_count_changes(self, project_data: Dict[str, Any]) -> bool:
        """ä¿®å¤é…éŸ³æ®µè½æ•°é‡å˜åŒ–é—®é¢˜"""
        try:
            logger.info("å¼€å§‹ä¿®å¤é…éŸ³æ®µè½æ•°é‡å˜åŒ–é—®é¢˜...")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨è‡ªåŠ¨æ•°é‡è°ƒæ•´
            voice_generation = project_data.get('voice_generation', {})
            voice_segments = voice_generation.get('voice_segments', [])
            
            # è·å–åŸå§‹æ–‡æœ¬æ®µè½æ•°é‡
            original_text = project_data.get('original_text', '')
            if original_text:
                original_paragraphs = self._split_text_to_paragraphs(original_text)
                original_count = len(original_paragraphs)
                current_count = len(voice_segments)
                
                logger.info(f"åŸå§‹æ®µè½æ•°: {original_count}, å½“å‰æ®µè½æ•°: {current_count}")
                
                if current_count != original_count:
                    # æ¢å¤åˆ°åŸå§‹æ®µè½æ•°é‡
                    logger.info(f"æ£€æµ‹åˆ°æ®µè½æ•°é‡å˜åŒ–ï¼Œæ¢å¤åˆ°åŸå§‹æ•°é‡: {original_count}")
                    restored_segments = self._restore_original_segments(
                        original_paragraphs, voice_segments
                    )
                    voice_generation['voice_segments'] = restored_segments
                    project_data['voice_generation'] = voice_generation
                    
                    # æ·»åŠ æ ‡è®°ï¼Œé˜²æ­¢è‡ªåŠ¨è°ƒæ•´
                    voice_generation['preserve_segment_count'] = True
            
            logger.info("é…éŸ³æ®µè½æ•°é‡ä¿®å¤å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿®å¤é…éŸ³æ®µè½æ•°é‡å¤±è´¥: {e}")
            return False
    
    def fix_description_content_errors(self, project_data: Dict[str, Any]) -> bool:
        """ä¿®å¤æè¿°å†…å®¹é”™è¯¯"""
        try:
            logger.info("å¼€å§‹ä¿®å¤æè¿°å†…å®¹é”™è¯¯...")
            
            # ä¿®å¤å›¾åƒç”Ÿæˆä¸­çš„æè¿°å†…å®¹
            image_generation = project_data.get('image_generation', {})
            generated_images = image_generation.get('generated_images', [])
            
            # è·å–é…éŸ³å†…å®¹ä½œä¸ºå‚è€ƒ
            voice_generation = project_data.get('voice_generation', {})
            voice_segments = voice_generation.get('voice_segments', [])
            
            # åˆ›å»ºé…éŸ³å†…å®¹æ˜ å°„
            voice_content_map = {}
            for segment in voice_segments:
                shot_id = segment.get('shot_id', '')
                content = segment.get('original_text', segment.get('dialogue_text', ''))
                if shot_id and content:
                    voice_content_map[shot_id] = content
            
            # ä¿®å¤å›¾åƒæè¿°
            fixed_images = []
            for image_data in generated_images:
                shot_id = image_data.get('shot_id', '')
                if shot_id in voice_content_map:
                    voice_content = voice_content_map[shot_id]
                    
                    # é‡æ–°ç”ŸæˆåŒ¹é…çš„æè¿°
                    new_description = self._generate_matched_description(voice_content)
                    if new_description:
                        image_data['consistency_description'] = new_description
                        image_data['enhanced_description'] = new_description
                        logger.info(f"ä¿®å¤æè¿°å†…å®¹: {shot_id}")
                
                fixed_images.append(image_data)
            
            image_generation['generated_images'] = fixed_images
            project_data['image_generation'] = image_generation
            
            logger.info("æè¿°å†…å®¹é”™è¯¯ä¿®å¤å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿®å¤æè¿°å†…å®¹é”™è¯¯å¤±è´¥: {e}")
            return False
    
    def fix_voice_time_image_generation(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®å¤æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒåŠŸèƒ½"""
        try:
            logger.info("å¼€å§‹ä¿®å¤æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒåŠŸèƒ½...")
            
            voice_generation = project_data.get('voice_generation', {})
            voice_segments = voice_generation.get('voice_segments', [])
            
            if not voice_segments:
                logger.warning("æ²¡æœ‰é…éŸ³æ®µè½æ•°æ®")
                return {}
            
            # åˆ†ææ¯ä¸ªé…éŸ³æ®µè½çš„æ—¶é•¿
            duration_analysis = {}
            total_duration = 0
            
            for i, segment in enumerate(voice_segments):
                audio_path = segment.get('audio_path', '')
                dialogue_text = segment.get('dialogue_text', segment.get('original_text', ''))
                
                # è·å–çœŸå®æ—¶é•¿
                if audio_path and os.path.exists(audio_path):
                    duration = self._get_real_audio_duration(audio_path)
                else:
                    # æ ¹æ®æ–‡æœ¬ä¼°ç®—æ—¶é•¿
                    duration = self._estimate_duration_from_text(dialogue_text)
                
                duration_analysis[i] = {
                    'segment_index': i,
                    'shot_id': segment.get('shot_id', f'é•œå¤´{i+1}'),
                    'scene_id': segment.get('scene_id', f'åœºæ™¯{(i//3)+1}'),
                    'duration': duration,
                    'content': dialogue_text,
                    'audio_path': audio_path
                }
                
                total_duration += duration
            
            # è®¡ç®—å›¾åƒéœ€æ±‚
            image_requirements = []
            total_images = 0
            
            for i, analysis in duration_analysis.items():
                duration = analysis['duration']
                
                # ğŸ”§ ä¿®æ”¹ï¼šæ¯ä¸ªé…éŸ³æ®µè½åªç”Ÿæˆ1å¼ å›¾ç‰‡ï¼Œç¡®ä¿é…éŸ³æ•°é‡ä¸å›¾ç‰‡æ•°é‡ä¸€è‡´
                image_count = 1
                
                # ç”Ÿæˆå›¾åƒéœ€æ±‚
                for img_idx in range(image_count):
                    image_req = {
                        'segment_index': i,
                        'image_index': img_idx,
                        'shot_id': f"{analysis['shot_id']}_{img_idx+1}" if image_count > 1 else analysis['shot_id'],
                        'scene_id': analysis['scene_id'],
                        'content': analysis['content'],
                        'duration_start': img_idx * (duration / image_count),
                        'duration_end': (img_idx + 1) * (duration / image_count),
                        'description': self._generate_matched_description(analysis['content'])
                    }
                    image_requirements.append(image_req)
                    total_images += 1
            
            result = {
                'voice_segments_count': len(voice_segments),
                'total_voice_duration': total_duration,
                'recommended_images_count': total_images,
                'duration_analysis': duration_analysis,
                'image_requirements': image_requirements,
                'analysis_summary': {
                    'short_segments': len([d for d in duration_analysis.values() if d['duration'] <= 3.0]),
                    'medium_segments': len([d for d in duration_analysis.values() if 3.0 < d['duration'] <= 6.0]),
                    'long_segments': len([d for d in duration_analysis.values() if d['duration'] > 6.0]),
                    'average_duration': total_duration / len(voice_segments) if voice_segments else 0
                }
            }
            
            logger.info(f"æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒåˆ†æå®Œæˆ: {len(voice_segments)}æ®µé…éŸ³ -> {total_images}å¼ å›¾åƒ")
            return result
            
        except Exception as e:
            logger.error(f"ä¿®å¤æŒ‰é…éŸ³æ—¶é—´ç”Ÿæˆå›¾åƒåŠŸèƒ½å¤±è´¥: {e}")
            return {}
    
    def _get_real_audio_duration(self, audio_path: str) -> float:
        """è·å–çœŸå®éŸ³é¢‘æ—¶é•¿"""
        try:
            if not audio_path or not os.path.exists(audio_path):
                return 0.0
            
            # ä½¿ç”¨éŸ³é¢‘åˆ†æå™¨è·å–ç²¾ç¡®æ—¶é•¿
            duration = self.audio_analyzer.analyze_duration(audio_path)
            return duration
            
        except Exception as e:
            logger.error(f"è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_duration_from_text(self, text: str) -> float:
        """æ ¹æ®æ–‡æœ¬ä¼°ç®—æ—¶é•¿"""
        if not text:
            return 3.0
        
        # ä¸­æ–‡æ¯ç§’4å­—ï¼Œè‹±æ–‡æ¯ç§’2.5è¯
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_words = len([word for word in text.split() if word.isalpha()])
        
        if chinese_chars > english_words:
            duration = chinese_chars / 4.0
        else:
            duration = english_words / 2.5
        
        # åº”ç”¨åœé¡¿å› å­å’Œé™åˆ¶èŒƒå›´
        duration = duration * 1.2  # åœé¡¿å› å­
        return max(1.0, min(duration, 30.0))
    
    def _split_text_to_paragraphs(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ®µè½"""
        if not text:
            return []
        
        # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        # å¦‚æœæ²¡æœ‰æ¢è¡Œç¬¦ï¼ŒæŒ‰å¥å·åˆ†å‰²
        if len(paragraphs) == 1:
            sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
            if len(sentences) > 1:
                paragraphs = [s + 'ã€‚' if not s.endswith('ã€‚') else s for s in sentences]
        
        return paragraphs
    
    def _restore_original_segments(self, original_paragraphs: List[str], 
                                 current_segments: List[Dict]) -> List[Dict]:
        """æ¢å¤åˆ°åŸå§‹æ®µè½æ•°é‡"""
        restored_segments = []
        
        for i, paragraph in enumerate(original_paragraphs):
            # å°è¯•æ‰¾åˆ°åŒ¹é…çš„ç°æœ‰æ®µè½
            matched_segment = None
            for segment in current_segments:
                if paragraph in segment.get('original_text', '') or \
                   segment.get('original_text', '') in paragraph:
                    matched_segment = segment
                    break
            
            if matched_segment:
                # ä½¿ç”¨åŒ¹é…çš„æ®µè½
                segment = matched_segment.copy()
                segment['original_text'] = paragraph
                segment['dialogue_text'] = paragraph
            else:
                # åˆ›å»ºæ–°æ®µè½
                segment = {
                    'index': i,
                    'scene_id': f'åœºæ™¯{(i//3)+1}',
                    'shot_id': f'é•œå¤´{i+1}',
                    'original_text': paragraph,
                    'dialogue_text': paragraph,
                    'sound_effect': '',
                    'status': 'æœªç”Ÿæˆ',
                    'audio_path': ''
                }
            
            restored_segments.append(segment)
        
        return restored_segments
    
    def _generate_matched_description(self, content: str) -> str:
        """ç”ŸæˆåŒ¹é…çš„æè¿°"""
        if not content:
            return "æ ¹æ®å†…å®¹ç”Ÿæˆç”»é¢"
        
        # ç®€å•çš„æè¿°ç”Ÿæˆé€»è¾‘
        # å®é™…åº”è¯¥ä½¿ç”¨LLMæˆ–æ›´å¤æ‚çš„ç®—æ³•
        return f"æ ¹æ®å†…å®¹ç”Ÿæˆç”»é¢: {content[:50]}{'...' if len(content) > 50 else ''}, é«˜è´¨é‡, ç»†èŠ‚ä¸°å¯Œ"
